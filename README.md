# Low-Rank Adaptation of Large Language ModelS

AIM -> finetune a LLM which acts like a copilot and complrtes the prompt in the language of python (completes the code) .

Approach -> We appliy LoRA adapters on top of Q/K/V linear layers in Llama attention .

Here used a small validation dataset of codeparrot dataset as our training set. It is available in hf and contains python code 

![image](https://github.com/user-attachments/assets/e7b093c3-b30d-403b-8748-889c0bd7e2b3)

# LORA




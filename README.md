# Low-Rank Adaptation of a Large Language Model 

The code applies LoRA adapters on top of Q/K/V linear layers in Llama attention

final aim is to finetune a LLM which acts like a copilot and complrtes the prompt in the language of python (completes the code) .

used a small validation dataset of codeparrot dataset available in hf 

![image](https://github.com/user-attachments/assets/e7b093c3-b30d-403b-8748-889c0bd7e2b3)



